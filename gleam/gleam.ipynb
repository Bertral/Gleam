{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gleam\n",
    "This notebook aims to design, train, validate, and use a convolutional neural network that is able to predict the distribution of population in a geographical area from nighttime satellite imagery. The product is a high resolution approximation map of the population distribution.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "[Recommended source for nighttime pictures (NOAA)](https://ngdc.noaa.gov/eog/viirs/download_dnb_composites.html)\n",
    "\n",
    "[Recommended source for population rasters (SEDAC)](http://sedac.ciesin.columbia.edu/data/set/gpw-v4-population-count-adjusted-to-2015-unwpp-country-totals-rev10)\n",
    "\n",
    "[Recommended vector file to isolate a country (Natural Earth)](https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/)\n",
    "\n",
    "This data needs to be processed with a GIS tool to meet a few requirements before training :\n",
    "- Rasters of the same year need to be merged in one GeoTIFF file : lights on the first band, population on the second band.\n",
    "- Since both sets don't have the same resolution, upscaling the population raster might have increased population density. This needs to be adjusted before training (use `rescale_pop.py` in the same folder as this notebook, don't forget to change some variables). In the case of the recommended datasets, every population pixel needs to be devided by 4 after merging with gdal_merge or QGIS.\n",
    "- Use a vector file to clip the raster and isolate the region of interest. Areas with a population of zero (out of borders or in the ocean) will be ignored during preprocessing.\n",
    "\n",
    "## Trained models\n",
    "A few already trained models can be found in `./models`. These were trained on the recommended datasets using the scripts below for the year 2015, so the input dataset for a prediction must have 1 pixel per 0.25 square kilometer. The name of the model indicates the contries that were used to train it (`safrica_namibia.h5` is South Africa and Namibia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import keras.layers.core as core\n",
    "import keras.layers.convolutional as conv\n",
    "import keras.models as models\n",
    "import keras.callbacks\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import optimizers\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The preprocess function creates a sliding window over the input raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filepath, input_tile_size, offset):\n",
    "    \"\"\"\n",
    "    :param filepath: GeoTIFF raster file with 2 bands\n",
    "    :param input_tile_size: width of the square sliding window, and of the output tiles\n",
    "    :param offset: distance in pixels the window will move between each tile\n",
    "    :return: (X, Y) where X is a suitable array of inputs for the neural network\n",
    "            and Y is the expected output for each of these inputs\n",
    "    This function filters out the tiles that have zero population on them.\n",
    "    \"\"\"\n",
    "    \n",
    "    raster = rasterio.open(filepath)\n",
    "\n",
    "    matrix_x = raster.read(1)\n",
    "    matrix_y = raster.read(2)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    col = 0\n",
    "    while col + input_tile_size < matrix_x.shape[1]:\n",
    "        row = 0\n",
    "        while row + input_tile_size < matrix_x.shape[0]:\n",
    "            pop = np.sum(matrix_y[row: row + input_tile_size, col: col + input_tile_size])\n",
    "            # only use tiles that have people living on it\n",
    "            if pop > 0:\n",
    "                X.append(matrix_x[row: row + input_tile_size, col: col + input_tile_size])\n",
    "                Y.append(pop)\n",
    "\n",
    "            row += offset\n",
    "        col += offset\n",
    "\n",
    "    raster.close()\n",
    "    matrix_x, matrix_y = None, None  # free some memory\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    X = np.expand_dims(X, axis=3)  # add the color channel as a new dimension\n",
    "    print('input shape (observations, obs_width, obs_height, channels) : ' + str(X.shape))\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network\n",
    "This part of the script defines how the neural network will be initialized and creates callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(input_shape):\n",
    "    # kernel size for each convolution layer\n",
    "    kernel_size = (3, 3)\n",
    "    \n",
    "    cnn = models.Sequential()\n",
    "\n",
    "    cnn.add(conv.Convolution2D(filters=64, kernel_size=kernel_size, activation=\"relu\", padding='same',\n",
    "                               input_shape=input_shape))\n",
    "    cnn.add(conv.AveragePooling2D(strides=(2, 2)))\n",
    "    \n",
    "    cnn.add(conv.Convolution2D(filters=128, kernel_size=kernel_size, activation=\"relu\", padding='same'))\n",
    "    cnn.add(conv.AveragePooling2D(strides=(2, 2)))\n",
    "\n",
    "    cnn.add(conv.Convolution2D(filters=256, kernel_size=kernel_size, activation=\"relu\", padding='same'))\n",
    "    cnn.add(conv.AveragePooling2D(strides=(2, 2)))\n",
    "    \n",
    "    cnn.add(core.Flatten())\n",
    "    cnn.add(core.Dropout(0.5))\n",
    "    cnn.add(core.Dense(128))\n",
    "    cnn.add(core.Dense(1))\n",
    "\n",
    "    cnn.compile(loss=\"mean_squared_error\", optimizer=optimizers.Adam(lr=0.02, decay=0.0), metrics=[\"mse\", \"mae\"])\n",
    "    return cnn\n",
    "\n",
    "# CALLBACKS\n",
    "# reduce learning rate when we stopped learning anything\n",
    "rlrp = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=20, verbose=1, mode='auto', min_lr=0.0000001)\n",
    "\n",
    "# stop learning early if we stopped leaning anything for a longer time\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=200, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold validation\n",
    "This script trains and validates the model using 4-fold cross-validation. It was used to find the best performing neural network topology and measure the amount of error we can expect from a trained model. The resulting model is saved to the models/ subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS ##\n",
    "nb_epoch = 2000  # maximum number of epochs\n",
    "model_birthday = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.gmtime())  # used to identify generated files (logs and models)\n",
    "raster_path = '../data/lightpop_merged/adj_2015_usa.tif'  # raster containing the training dataset\n",
    "verbose = 2  # 0: no progress message, 1: progress bar for each epoch, 2: one message for each epoch\n",
    "################\n",
    "\n",
    "# CALLBACKS\n",
    "# logs for tensorboard\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"logs/\" + model_birthday)\n",
    "\n",
    "print('preprocessing')\n",
    "X, Y = preprocess(raster_path, 32, 32)\n",
    "\n",
    "print('configuring cnn')\n",
    "\n",
    "# input dimensions\n",
    "img_count, img_rows, img_cols, img_channel_count = X.shape\n",
    "\n",
    "# k-fold split\n",
    "kfold = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "\n",
    "# initialize statistics aggregate\n",
    "kfold_mse = []\n",
    "kfold_mae = []\n",
    "kfold_sae = []\n",
    "\n",
    "print('logs will be saved to logs/' + model_birthday)\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    cnn = init_cnn((img_rows, img_cols, img_channel_count))\n",
    "\n",
    "    print('training ...')\n",
    "\n",
    "    cnn.fit(X[train], Y[train], batch_size=1024, epochs=nb_epoch, verbose=verbose,\n",
    "            callbacks=[tensorboard, rlrp, early_stopping],\n",
    "            sample_weight=None)\n",
    "\n",
    "    cnn.save('models/' + model_birthday + '.h5')\n",
    "\n",
    "    print('model saved to models/' + model_birthday + '.h5')\n",
    "    \n",
    "    # evaluate and print stats\n",
    "    evaluation = cnn.evaluate(X[test], Y[test], verbose=2, batch_size=1024)\n",
    "    evaluation = dict(zip(cnn.metrics_names, evaluation))\n",
    "    kfold_mse.append(evaluation['mean_squared_error'])\n",
    "    kfold_mae.append(evaluation['mean_absolute_error'])\n",
    "    kfold_sae.append(evaluation['mean_absolute_error'] * len(Y[test]))\n",
    "    \n",
    "    print('K-fold validation results :')\n",
    "    print('Mean squared error : %.2f (std %.2f)' % (np.mean(kfold_mse), np.std(kfold_mse)))\n",
    "    print('Mean absolute error : %.2f (std %.2f)' % (np.mean(kfold_mae), np.std(kfold_mae)))\n",
    "    print('Sum of absolute errors : %.2f (std %.2f)' % (np.mean(kfold_sae), np.std(kfold_sae)))\n",
    "\n",
    "print('done !')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without validation\n",
    "Trains on a whole raster once, without validating. The goal is to build a model that makes the best possible prediction, so it has to train on every data available including the validation set. The resulting model is saved to the `./models` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS ##\n",
    "nb_epoch = 5000  # maximum number of epochs\n",
    "model_birthday = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.gmtime())  # used to identify generated files (logs and models)\n",
    "raster_path = '../data/lightpop_merged/adj_2015_usa.tif'  # raster containing the training dataset\n",
    "verbose = 2  # 0: no progress message, 1: progress bar for each epoch, 2: one message for each epoch\n",
    "################\n",
    "\n",
    "# CALLBACKS\n",
    "# logs for tensorboard\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"logs/\" + model_birthday)\n",
    "\n",
    "# checkpoints in case of crash or interruption\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('models/' + model_birthday + '.h5', save_weights_only=False)\n",
    "\n",
    "print('preprocessing')\n",
    "\n",
    "X, Y = preprocess(raster_path, 32, 8)\n",
    "\n",
    "print('configuring cnn')\n",
    "\n",
    "# input dimensions\n",
    "img_count, img_rows, img_cols, img_channel_count = X.shape\n",
    "\n",
    "print('logs will be saved to logs/' + model_birthday)\n",
    "\n",
    "cnn = init_cnn((img_rows, img_cols, img_channel_count))\n",
    "\n",
    "print('training ...')\n",
    "\n",
    "cnn.fit(X, Y, batch_size=1024, epochs=nb_epoch, verbose=verbose,\n",
    "        callbacks=[tensorboard, checkpoint, rlrp, early_stopping], sample_weight=None)\n",
    "\n",
    "cnn.save('models/' + model_birthday + '.h5')\n",
    "\n",
    "print('model saved to models/' + model_birthday + '.h5')\n",
    "\n",
    "print('done !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "This script generates a high resolution population raster from a satellite image and a trained model. It uses its own preprocessing algorithm that works without validation data.\n",
    "\n",
    "Since the model gives only one output for each 32x32 tile, the distribution of the output value in the tile is managed by this script. It assumes a log relation between nightlights and population for each pixel.\n",
    "\n",
    "The notebook \"rastercomparator\" can then be used to compare actual data with predicted data, as well as the predicted population counts over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARAMETERS ##\n",
    "model_path = \"models/usa.h5\"\n",
    "prediction_dataset = '../data/lightpop_merged/adj_2015_portugal.tif'  # input nightlights\n",
    "out = '2015_brazil_to_2015_portugal.tif'  # output population file\n",
    "input_tile_size = 32\n",
    "################\n",
    "\n",
    "print('loading model')\n",
    "\n",
    "cnn = models.load_model(model_path)\n",
    "\n",
    "print('opening raster')\n",
    "\n",
    "raster = rasterio.open(prediction_dataset)\n",
    "band = raster.read(1)\n",
    "profile = raster.profile\n",
    "profile.update(count=1)\n",
    "width, height = raster.width, raster.height\n",
    "\n",
    "# preprocess\n",
    "matrix_x = raster.read(1)\n",
    "tiles_x = []\n",
    "y = 0\n",
    "while y + input_tile_size < matrix_x.shape[1]:\n",
    "    x = 0\n",
    "    while x + input_tile_size < matrix_x.shape[0]:\n",
    "        tiles_x.append(matrix_x[x: x + input_tile_size, y: y + input_tile_size])\n",
    "        x += input_tile_size\n",
    "    y += input_tile_size\n",
    "testX = np.array(tiles_x)\n",
    "raster.close()\n",
    "matrix_x = None\n",
    "tiles_x = None\n",
    "testX = np.expand_dims(testX, axis=3)\n",
    "\n",
    "print('generating raster')\n",
    "\n",
    "predicted_tiles = cnn.predict(testX, verbose=0)\n",
    "\n",
    "# rebuild raster from predictions\n",
    "predicted_raster = np.zeros(shape=(raster.height, raster.width))\n",
    "y = 0\n",
    "pred_index = 0\n",
    "while y + input_tile_size < width:\n",
    "    x = 0\n",
    "    while x + input_tile_size < height:\n",
    "        in_tile = band[x: x + input_tile_size, y: y + input_tile_size]\n",
    "        if np.max(in_tile) <= 0:\n",
    "            # avoid divisions by 0\n",
    "            predicted_raster[x: x + input_tile_size, y: y + input_tile_size] = 0\n",
    "        else:\n",
    "            # normalize visible light between 0 and 1 to avoid overflows (also gives better results)\n",
    "            weights = in_tile / np.max(in_tile)\n",
    "            # visible light is perceived logarithmically => counteract with exp\n",
    "            weights = np.exp(weights) - 1\n",
    "            # the sum of all weights must be 1\n",
    "            weights = weights / np.sum(weights)\n",
    "            predicted_raster[x: x + input_tile_size, y: y + input_tile_size] = predicted_tiles[pred_index] * weights\n",
    "\n",
    "        pred_index += 1\n",
    "        x += input_tile_size\n",
    "    y += input_tile_size\n",
    "\n",
    "predicted_raster = np.array(predicted_raster)\n",
    "\n",
    "with rasterio.open('predictions/' + out, 'w', **profile) as dst:\n",
    "    dst.write(predicted_raster.astype(rasterio.float32), 1)\n",
    "print(\"prediction saved to predictions/\" + out)\n",
    "\n",
    "predicted_raster = None\n",
    "\n",
    "print('prediction done !')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
